<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>深入 Transformer 腹地：Cross-Attention、MLP 与残差连接的化学反应<!-- --> | Notes</title><meta name="description" content="用代码和直觉类比拆解 Transformer 里最容易卡住的两个问题：Cross-Attention 如何通信，以及 MLP 为什么不会破坏上下文语义。"/><meta name="next-head-count" content="4"/><link data-next-font="" rel="preconnect" href="/" crossorigin="anonymous"/><link rel="preload" href="/_next/static/css/1a07419698fd4dbb.css" as="style"/><link rel="stylesheet" href="/_next/static/css/1a07419698fd4dbb.css" data-n-g=""/><link rel="preload" href="/_next/static/css/9e85d0ded3c4ad0c.css" as="style"/><link rel="stylesheet" href="/_next/static/css/9e85d0ded3c4ad0c.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-8fa1640cc84ba8fe.js" defer=""></script><script src="/_next/static/chunks/framework-2c79e2a64abdb08b.js" defer=""></script><script src="/_next/static/chunks/main-2f7c9761d58c33d5.js" defer=""></script><script src="/_next/static/chunks/pages/_app-60613eae8b6d608e.js" defer=""></script><script src="/_next/static/chunks/996-502a5e68cb1e9d91.js" defer=""></script><script src="/_next/static/chunks/pages/notes/%5Bslug%5D-e80d2ab2056b0109.js" defer=""></script><script src="/_next/static/QJd0GjVH6xVJhZQaEuL9F/_buildManifest.js" defer=""></script><script src="/_next/static/QJd0GjVH6xVJhZQaEuL9F/_ssgManifest.js" defer=""></script></head><body><div id="__next"><main class="Content_page__1z6He"><header class="Content_topbar__D6oJ8"><a class="Content_brand__QK3KR" href="/">littlemoon</a><nav class="Content_nav__dJMkc"><a href="/notes">Notes</a><a href="/demos">Demos</a></nav><div class="Content_themeToggle__jY6wj" role="group" aria-label="Color theme"><button type="button" class="Content_themeButton__jf_QP Content_themeButtonActive__FMvRi" aria-pressed="true">Auto</button><button type="button" class="Content_themeButton__jf_QP " aria-pressed="false">Light</button><button type="button" class="Content_themeButton__jf_QP " aria-pressed="false">Dark</button></div></header><article class="Content_shell__BTTGr"><section class="Content_article__HRTX5"><a class="Content_backLink__XMz_i" href="/notes">← Back to notes</a><h1 class="Content_articleTitle__l7aNW">深入 Transformer 腹地：Cross-Attention、MLP 与残差连接的化学反应</h1><p class="Content_articleSummary__7O5NT">用代码和直觉类比拆解 Transformer 里最容易卡住的两个问题：Cross-Attention 如何通信，以及 MLP 为什么不会破坏上下文语义。</p><div class="Content_articleMeta__lNsKM"><span>2026年2月26日</span><span class="Content_tag__islRM">transformer</span><span class="Content_tag__islRM">attention</span><span class="Content_tag__islRM">deep-learning</span></div><section class="Content_content__JpleF"><h1 id="深入-transformer-腹地cross-attentionmlp-与残差连接的化学反应">深入 Transformer 腹地：Cross-Attention、MLP 与残差连接的化学反应<a class="heading-anchor" aria-hidden="true" tabindex="-1" href="#深入-transformer-腹地cross-attentionmlp-与残差连接的化学反应">#</a></h1>
<p>在初学 Transformer 时，最容易卡壳的地方往往是结构之间的缝隙：特征是怎么流动的？这些结构拼在一起到底在做什么？</p>
<p>这篇笔记用手写版 <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e"><span data-line=""><span>microgpt.py</span></span></code></span>（完整 Encoder-Decoder 结构）来解释两个高频盲区。</p>
<h2 id="盲区一原版翻译模型的灵魂通道cross-attention">盲区一：原版翻译模型的灵魂通道（Cross-Attention）<a class="heading-anchor" aria-hidden="true" tabindex="-1" href="#盲区一原版翻译模型的灵魂通道cross-attention">#</a></h2>
<p>GPT 去掉了 Encoder，只保留 Decoder 做自回归生成。但在原版翻译场景中，Encoder 和 Decoder 必须通信。</p>
<p>答案就是 Cross-Attention。</p>
<h3 id="直觉模型翻译官向情报库提问">直觉模型：翻译官向情报库提问<a class="heading-anchor" aria-hidden="true" tabindex="-1" href="#直觉模型翻译官向情报库提问">#</a></h3>
<p>Encoder 把源文本编码成 <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e"><span data-line=""><span>enc_memory</span></span></code></span>，Decoder 在生成下一个词时，基于当前状态形成 Query，再去 <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e"><span data-line=""><span>enc_memory</span></span></code></span> 检索相关信息。</p>
<figure data-rehype-pretty-code-figure=""><pre style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e" tabindex="0" data-language="python" data-theme="github-light github-dark"><code data-language="python" data-theme="github-light github-dark" style="display: grid;"><span data-line=""><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D"># Decoder Cross-Attention</span></span>
<span data-line=""><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">cross_q </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> linear(x, state_dict[</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">f</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF">'dec_layer</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">li</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF">.cross_wq'</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">])</span></span>
<span data-line=""><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">enc_k_seq </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> [linear(mem, state_dict[</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">f</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF">'dec_layer</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">li</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF">.cross_wk'</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">]) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> mem </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> enc_memory]</span></span>
<span data-line=""><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">enc_v_seq </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> [linear(mem, state_dict[</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">f</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF">'dec_layer</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">li</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF">.cross_wv'</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">]) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> mem </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> enc_memory]</span></span></code></pre></figure>
<p>关键点：Q、K、V 可以来自不同来源并经过不同投影矩阵。</p>
<ul>
<li><span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e"><span data-line=""><span>Q</span></span></code></span>：来自 Decoder 当前状态（当前“想问什么”）</li>
<li><span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e"><span data-line=""><span>K</span></span></code></span>：来自 Encoder Memory（“怎么索引”）</li>
<li><span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e"><span data-line=""><span>V</span></span></code></span>：来自 Encoder Memory（“具体拿什么内容”）</li>
</ul>
<p>Attention 的本质是：把不同特征映射到可匹配的语义空间，再按权重提取信息。</p>
<h2 id="盲区二为什么-mlp-不会破坏-attention-的成果">盲区二：为什么 MLP 不会破坏 Attention 的成果？<a class="heading-anchor" aria-hidden="true" tabindex="-1" href="#盲区二为什么-mlp-不会破坏-attention-的成果">#</a></h2>
<p>每层 Transformer 常见流程是：</p>
<ol>
<li>Self-Attention：token 间信息交互</li>
<li>MLP：token 内部的非线性变换</li>
</ol>
<p>常见疑问：Attention 好不容易建立了上下文关系，MLP 会不会把它打散？</p>
<p>答案是不会，核心有两层原因。</p>
<h3 id="原因-1高维空间中的增量表达">原因 1：高维空间中的增量表达<a class="heading-anchor" aria-hidden="true" tabindex="-1" href="#原因-1高维空间中的增量表达">#</a></h3>
<p>Token 表示是高维向量。Attention 可能强化了某些维度上的关系，MLP 更像是在其他维度补充更抽象、更非线性的特征，而不是简单“覆盖旧信息”。</p>
<h3 id="原因-2残差连接保证信息底本被保留">原因 2：残差连接保证信息底本被保留<a class="heading-anchor" aria-hidden="true" tabindex="-1" href="#原因-2残差连接保证信息底本被保留">#</a></h3>
<figure data-rehype-pretty-code-figure=""><pre style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e" tabindex="0" data-language="python" data-theme="github-light github-dark"><code data-language="python" data-theme="github-light github-dark" style="display: grid;"><span data-line=""><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">x_residual </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> x</span></span>
<span data-line=""><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> mlp(x)</span></span>
<span data-line=""><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">+</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> x_residual</span></span></code></pre></figure>
<p>这是“在原有信息上做增量修正”，不是“把原信息替换掉”。</p>
<p>如果初始向量是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">X_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，那么每层更像是在累积增量：</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>X</mi><mrow><mi>f</mi><mi>i</mi><mi>n</mi><mi>a</mi><mi>l</mi></mrow></msub><mo>=</mo><msub><mi>X</mi><mn>0</mn></msub><mo>+</mo><mo stretchy="false">(</mo><mi mathvariant="normal">Δ</mi><msub><mi>A</mi><mn>1</mn></msub><mo>+</mo><mi mathvariant="normal">Δ</mi><msub><mi>M</mi><mn>1</mn></msub><mo stretchy="false">)</mo><mo>+</mo><mo>⋯</mo><mo>+</mo><mo stretchy="false">(</mo><mi mathvariant="normal">Δ</mi><msub><mi>A</mi><mi>n</mi></msub><mo>+</mo><mi mathvariant="normal">Δ</mi><msub><mi>M</mi><mi>n</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">X_{final} = X_0 + (\Delta A_1 + \Delta M_1) + \dots + (\Delta A_n + \Delta M_n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10764em;">f</span><span class="mord mathnormal mtight">ina</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">Δ</span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">Δ</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.109em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="minner">⋯</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">Δ</span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">Δ</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.109em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>
<p>其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Δ</mi><msub><mi>A</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\Delta A_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord">Δ</span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 是第 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span> 层 Attention 的增量，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Δ</mi><msub><mi>M</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\Delta M_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord">Δ</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.109em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 是第 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span> 层 MLP 的增量。</p>
<h2 id="一句话总结">一句话总结<a class="heading-anchor" aria-hidden="true" tabindex="-1" href="#一句话总结">#</a></h2>
<p>Transformer 的深层可训练性来自一个很朴素的工程哲学：不要粗暴覆写上一层表示，而是在残差通道上持续做可控增量。</p></section></section></article></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"note":{"slug":"transformer-cross-attention-mlp-residual","title":"深入 Transformer 腹地：Cross-Attention、MLP 与残差连接的化学反应","date":"2026-02-26","summary":"用代码和直觉类比拆解 Transformer 里最容易卡住的两个问题：Cross-Attention 如何通信，以及 MLP 为什么不会破坏上下文语义。","draft":false,"tags":["transformer","attention","deep-learning"],"html":"\u003ch1 id=\"深入-transformer-腹地cross-attentionmlp-与残差连接的化学反应\"\u003e深入 Transformer 腹地：Cross-Attention、MLP 与残差连接的化学反应\u003ca class=\"heading-anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#深入-transformer-腹地cross-attentionmlp-与残差连接的化学反应\"\u003e#\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003e在初学 Transformer 时，最容易卡壳的地方往往是结构之间的缝隙：特征是怎么流动的？这些结构拼在一起到底在做什么？\u003c/p\u003e\n\u003cp\u003e这篇笔记用手写版 \u003cspan data-rehype-pretty-code-figure=\"\"\u003e\u003ccode data-language=\"text\" data-theme=\"github-light github-dark\" style=\"--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e\"\u003e\u003cspan data-line=\"\"\u003e\u003cspan\u003emicrogpt.py\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/span\u003e（完整 Encoder-Decoder 结构）来解释两个高频盲区。\u003c/p\u003e\n\u003ch2 id=\"盲区一原版翻译模型的灵魂通道cross-attention\"\u003e盲区一：原版翻译模型的灵魂通道（Cross-Attention）\u003ca class=\"heading-anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#盲区一原版翻译模型的灵魂通道cross-attention\"\u003e#\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eGPT 去掉了 Encoder，只保留 Decoder 做自回归生成。但在原版翻译场景中，Encoder 和 Decoder 必须通信。\u003c/p\u003e\n\u003cp\u003e答案就是 Cross-Attention。\u003c/p\u003e\n\u003ch3 id=\"直觉模型翻译官向情报库提问\"\u003e直觉模型：翻译官向情报库提问\u003ca class=\"heading-anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#直觉模型翻译官向情报库提问\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eEncoder 把源文本编码成 \u003cspan data-rehype-pretty-code-figure=\"\"\u003e\u003ccode data-language=\"text\" data-theme=\"github-light github-dark\" style=\"--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e\"\u003e\u003cspan data-line=\"\"\u003e\u003cspan\u003eenc_memory\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/span\u003e，Decoder 在生成下一个词时，基于当前状态形成 Query，再去 \u003cspan data-rehype-pretty-code-figure=\"\"\u003e\u003ccode data-language=\"text\" data-theme=\"github-light github-dark\" style=\"--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e\"\u003e\u003cspan data-line=\"\"\u003e\u003cspan\u003eenc_memory\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/span\u003e 检索相关信息。\u003c/p\u003e\n\u003cfigure data-rehype-pretty-code-figure=\"\"\u003e\u003cpre style=\"--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e\" tabindex=\"0\" data-language=\"python\" data-theme=\"github-light github-dark\"\u003e\u003ccode data-language=\"python\" data-theme=\"github-light github-dark\" style=\"display: grid;\"\u003e\u003cspan data-line=\"\"\u003e\u003cspan style=\"--shiki-light:#6A737D;--shiki-dark:#6A737D\"\u003e# Decoder Cross-Attention\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\"\u003ecross_q \u003c/span\u003e\u003cspan style=\"--shiki-light:#D73A49;--shiki-dark:#F97583\"\u003e=\u003c/span\u003e\u003cspan style=\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\"\u003e linear(x, state_dict[\u003c/span\u003e\u003cspan style=\"--shiki-light:#D73A49;--shiki-dark:#F97583\"\u003ef\u003c/span\u003e\u003cspan style=\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\"\u003e'dec_layer\u003c/span\u003e\u003cspan style=\"--shiki-light:#005CC5;--shiki-dark:#79B8FF\"\u003e{\u003c/span\u003e\u003cspan style=\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\"\u003eli\u003c/span\u003e\u003cspan style=\"--shiki-light:#005CC5;--shiki-dark:#79B8FF\"\u003e}\u003c/span\u003e\u003cspan style=\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\"\u003e.cross_wq'\u003c/span\u003e\u003cspan style=\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\"\u003e])\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\"\u003eenc_k_seq \u003c/span\u003e\u003cspan style=\"--shiki-light:#D73A49;--shiki-dark:#F97583\"\u003e=\u003c/span\u003e\u003cspan style=\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\"\u003e [linear(mem, state_dict[\u003c/span\u003e\u003cspan style=\"--shiki-light:#D73A49;--shiki-dark:#F97583\"\u003ef\u003c/span\u003e\u003cspan style=\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\"\u003e'dec_layer\u003c/span\u003e\u003cspan style=\"--shiki-light:#005CC5;--shiki-dark:#79B8FF\"\u003e{\u003c/span\u003e\u003cspan style=\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\"\u003eli\u003c/span\u003e\u003cspan style=\"--shiki-light:#005CC5;--shiki-dark:#79B8FF\"\u003e}\u003c/span\u003e\u003cspan style=\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\"\u003e.cross_wk'\u003c/span\u003e\u003cspan style=\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\"\u003e]) \u003c/span\u003e\u003cspan style=\"--shiki-light:#D73A49;--shiki-dark:#F97583\"\u003efor\u003c/span\u003e\u003cspan style=\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\"\u003e mem \u003c/span\u003e\u003cspan style=\"--shiki-light:#D73A49;--shiki-dark:#F97583\"\u003ein\u003c/span\u003e\u003cspan style=\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\"\u003e enc_memory]\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\"\u003eenc_v_seq \u003c/span\u003e\u003cspan style=\"--shiki-light:#D73A49;--shiki-dark:#F97583\"\u003e=\u003c/span\u003e\u003cspan style=\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\"\u003e [linear(mem, state_dict[\u003c/span\u003e\u003cspan style=\"--shiki-light:#D73A49;--shiki-dark:#F97583\"\u003ef\u003c/span\u003e\u003cspan style=\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\"\u003e'dec_layer\u003c/span\u003e\u003cspan style=\"--shiki-light:#005CC5;--shiki-dark:#79B8FF\"\u003e{\u003c/span\u003e\u003cspan style=\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\"\u003eli\u003c/span\u003e\u003cspan style=\"--shiki-light:#005CC5;--shiki-dark:#79B8FF\"\u003e}\u003c/span\u003e\u003cspan style=\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\"\u003e.cross_wv'\u003c/span\u003e\u003cspan style=\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\"\u003e]) \u003c/span\u003e\u003cspan style=\"--shiki-light:#D73A49;--shiki-dark:#F97583\"\u003efor\u003c/span\u003e\u003cspan style=\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\"\u003e mem \u003c/span\u003e\u003cspan style=\"--shiki-light:#D73A49;--shiki-dark:#F97583\"\u003ein\u003c/span\u003e\u003cspan style=\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\"\u003e enc_memory]\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/figure\u003e\n\u003cp\u003e关键点：Q、K、V 可以来自不同来源并经过不同投影矩阵。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cspan data-rehype-pretty-code-figure=\"\"\u003e\u003ccode data-language=\"text\" data-theme=\"github-light github-dark\" style=\"--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e\"\u003e\u003cspan data-line=\"\"\u003e\u003cspan\u003eQ\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/span\u003e：来自 Decoder 当前状态（当前“想问什么”）\u003c/li\u003e\n\u003cli\u003e\u003cspan data-rehype-pretty-code-figure=\"\"\u003e\u003ccode data-language=\"text\" data-theme=\"github-light github-dark\" style=\"--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e\"\u003e\u003cspan data-line=\"\"\u003e\u003cspan\u003eK\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/span\u003e：来自 Encoder Memory（“怎么索引”）\u003c/li\u003e\n\u003cli\u003e\u003cspan data-rehype-pretty-code-figure=\"\"\u003e\u003ccode data-language=\"text\" data-theme=\"github-light github-dark\" style=\"--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e\"\u003e\u003cspan data-line=\"\"\u003e\u003cspan\u003eV\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/span\u003e：来自 Encoder Memory（“具体拿什么内容”）\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAttention 的本质是：把不同特征映射到可匹配的语义空间，再按权重提取信息。\u003c/p\u003e\n\u003ch2 id=\"盲区二为什么-mlp-不会破坏-attention-的成果\"\u003e盲区二：为什么 MLP 不会破坏 Attention 的成果？\u003ca class=\"heading-anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#盲区二为什么-mlp-不会破坏-attention-的成果\"\u003e#\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003e每层 Transformer 常见流程是：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eSelf-Attention：token 间信息交互\u003c/li\u003e\n\u003cli\u003eMLP：token 内部的非线性变换\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e常见疑问：Attention 好不容易建立了上下文关系，MLP 会不会把它打散？\u003c/p\u003e\n\u003cp\u003e答案是不会，核心有两层原因。\u003c/p\u003e\n\u003ch3 id=\"原因-1高维空间中的增量表达\"\u003e原因 1：高维空间中的增量表达\u003ca class=\"heading-anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#原因-1高维空间中的增量表达\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eToken 表示是高维向量。Attention 可能强化了某些维度上的关系，MLP 更像是在其他维度补充更抽象、更非线性的特征，而不是简单“覆盖旧信息”。\u003c/p\u003e\n\u003ch3 id=\"原因-2残差连接保证信息底本被保留\"\u003e原因 2：残差连接保证信息底本被保留\u003ca class=\"heading-anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#原因-2残差连接保证信息底本被保留\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cfigure data-rehype-pretty-code-figure=\"\"\u003e\u003cpre style=\"--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e\" tabindex=\"0\" data-language=\"python\" data-theme=\"github-light github-dark\"\u003e\u003ccode data-language=\"python\" data-theme=\"github-light github-dark\" style=\"display: grid;\"\u003e\u003cspan data-line=\"\"\u003e\u003cspan style=\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\"\u003ex_residual \u003c/span\u003e\u003cspan style=\"--shiki-light:#D73A49;--shiki-dark:#F97583\"\u003e=\u003c/span\u003e\u003cspan style=\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\"\u003e x\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\"\u003ex \u003c/span\u003e\u003cspan style=\"--shiki-light:#D73A49;--shiki-dark:#F97583\"\u003e=\u003c/span\u003e\u003cspan style=\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\"\u003e mlp(x)\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\"\u003ex \u003c/span\u003e\u003cspan style=\"--shiki-light:#D73A49;--shiki-dark:#F97583\"\u003e=\u003c/span\u003e\u003cspan style=\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\"\u003e x \u003c/span\u003e\u003cspan style=\"--shiki-light:#D73A49;--shiki-dark:#F97583\"\u003e+\u003c/span\u003e\u003cspan style=\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\"\u003e x_residual\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/figure\u003e\n\u003cp\u003e这是“在原有信息上做增量修正”，不是“把原信息替换掉”。\u003c/p\u003e\n\u003cp\u003e如果初始向量是 \u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003eX\u003c/mi\u003e\u003cmn\u003e0\u003c/mn\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eX_0\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.07847em;\"\u003eX\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.3011em;\"\u003e\u003cspan style=\"top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e0\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.15em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e，那么每层更像是在累积增量：\u003c/p\u003e\n\u003cspan class=\"katex-display\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003eX\u003c/mi\u003e\u003cmrow\u003e\u003cmi\u003ef\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmi\u003en\u003c/mi\u003e\u003cmi\u003ea\u003c/mi\u003e\u003cmi\u003el\u003c/mi\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003eX\u003c/mi\u003e\u003cmn\u003e0\u003c/mn\u003e\u003c/msub\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi mathvariant=\"normal\"\u003eΔ\u003c/mi\u003e\u003cmsub\u003e\u003cmi\u003eA\u003c/mi\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/msub\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmi mathvariant=\"normal\"\u003eΔ\u003c/mi\u003e\u003cmsub\u003e\u003cmi\u003eM\u003c/mi\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmo\u003e⋯\u003c/mo\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi mathvariant=\"normal\"\u003eΔ\u003c/mi\u003e\u003cmsub\u003e\u003cmi\u003eA\u003c/mi\u003e\u003cmi\u003en\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmi mathvariant=\"normal\"\u003eΔ\u003c/mi\u003e\u003cmsub\u003e\u003cmi\u003eM\u003c/mi\u003e\u003cmi\u003en\u003c/mi\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eX_{final} = X_0 + (\\Delta A_1 + \\Delta M_1) + \\dots + (\\Delta A_n + \\Delta M_n)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.9694em;vertical-align:-0.2861em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.07847em;\"\u003eX\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.3361em;\"\u003e\u003cspan style=\"top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.10764em;\"\u003ef\u003c/span\u003e\u003cspan class=\"mord mathnormal mtight\"\u003eina\u003c/span\u003e\u003cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.01968em;\"\u003el\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.2861em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e=\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.07847em;\"\u003eX\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.3011em;\"\u003e\u003cspan style=\"top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e0\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.15em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e+\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mopen\"\u003e(\u003c/span\u003e\u003cspan class=\"mord\"\u003eΔ\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\"\u003eA\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.3011em;\"\u003e\u003cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e1\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.15em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e+\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003eΔ\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.10903em;\"\u003eM\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.3011em;\"\u003e\u003cspan style=\"top:-2.55em;margin-left:-0.109em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e1\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.15em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mclose\"\u003e)\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e+\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6667em;vertical-align:-0.0833em;\"\u003e\u003c/span\u003e\u003cspan class=\"minner\"\u003e⋯\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e+\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mopen\"\u003e(\u003c/span\u003e\u003cspan class=\"mord\"\u003eΔ\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\"\u003eA\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.1514em;\"\u003e\u003cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\"\u003en\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.15em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e+\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003eΔ\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.10903em;\"\u003eM\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.1514em;\"\u003e\u003cspan style=\"top:-2.55em;margin-left:-0.109em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\"\u003en\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.15em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mclose\"\u003e)\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\n\u003cp\u003e其中 \u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi mathvariant=\"normal\"\u003eΔ\u003c/mi\u003e\u003cmsub\u003e\u003cmi\u003eA\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\Delta A_i\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003eΔ\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\"\u003eA\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.3117em;\"\u003e\u003cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\"\u003ei\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.15em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e 是第 \u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ei\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6595em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ei\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e 层 Attention 的增量，\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi mathvariant=\"normal\"\u003eΔ\u003c/mi\u003e\u003cmsub\u003e\u003cmi\u003eM\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\Delta M_i\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003eΔ\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.10903em;\"\u003eM\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.3117em;\"\u003e\u003cspan style=\"top:-2.55em;margin-left:-0.109em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\"\u003ei\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.15em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e 是第 \u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ei\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6595em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ei\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e 层 MLP 的增量。\u003c/p\u003e\n\u003ch2 id=\"一句话总结\"\u003e一句话总结\u003ca class=\"heading-anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#一句话总结\"\u003e#\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eTransformer 的深层可训练性来自一个很朴素的工程哲学：不要粗暴覆写上一层表示，而是在残差通道上持续做可控增量。\u003c/p\u003e"}},"__N_SSG":true},"page":"/notes/[slug]","query":{"slug":"transformer-cross-attention-mlp-residual"},"buildId":"QJd0GjVH6xVJhZQaEuL9F","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>