{"pageProps":{"notes":[{"slug":"transformer-cross-attention-mlp-residual","title":"深入 Transformer 腹地：Cross-Attention、MLP 与残差连接的化学反应","date":"2026-02-26","summary":"用代码和直觉类比拆解 Transformer 里最容易卡住的两个问题：Cross-Attention 如何通信，以及 MLP 为什么不会破坏上下文语义。","draft":false,"tags":["transformer","attention","deep-learning"]}],"demos":[{"slug":"prompt-playground","title":"Prompt Playground","date":"2026-02-26","summary":"对比不同提示模板在同一任务下的输出质量，快速收敛可复用写法。","draft":false,"stack":["nextjs","typescript","prompt-design"],"liveUrl":"https://example.com/prompt-playground","repoUrl":"https://github.com/littlemoon-zh/prompt-playground"},{"slug":"visualization-sandbox","title":"Visualization Sandbox","date":"2026-02-25","summary":"通过同一组数据快速验证不同图表与叙事方式，找到更清晰的信息表达。","draft":false,"stack":["dataviz","d3","storytelling"],"liveUrl":"https://example.com/visualization-sandbox","repoUrl":"https://github.com/littlemoon-zh/visualization-sandbox"}]},"__N_SSG":true}