{"pageProps":{"note":{"slug":"transformer-cross-attention-mlp-residual","title":"深入 Transformer 腹地：Cross-Attention、MLP 与残差连接的化学反应","date":"2026-02-26","summary":"用代码和直觉类比拆解 Transformer 里最容易卡住的两个问题：Cross-Attention 如何通信，以及 MLP 为什么不会破坏上下文语义。","draft":false,"tags":["transformer","attention","deep-learning"],"html":"<h1 id=\"深入-transformer-腹地cross-attentionmlp-与残差连接的化学反应\">深入 Transformer 腹地：Cross-Attention、MLP 与残差连接的化学反应<a class=\"heading-anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#深入-transformer-腹地cross-attentionmlp-与残差连接的化学反应\">#</a></h1>\n<p>在初学 Transformer 时，最容易卡壳的地方往往是结构之间的缝隙：特征是怎么流动的？这些结构拼在一起到底在做什么？</p>\n<p>这篇笔记用手写版 <span data-rehype-pretty-code-figure=\"\"><code data-language=\"text\" data-theme=\"github-light github-dark\" style=\"--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e\"><span data-line=\"\"><span>microgpt.py</span></span></code></span>（完整 Encoder-Decoder 结构）来解释两个高频盲区。</p>\n<h2 id=\"盲区一原版翻译模型的灵魂通道cross-attention\">盲区一：原版翻译模型的灵魂通道（Cross-Attention）<a class=\"heading-anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#盲区一原版翻译模型的灵魂通道cross-attention\">#</a></h2>\n<p>GPT 去掉了 Encoder，只保留 Decoder 做自回归生成。但在原版翻译场景中，Encoder 和 Decoder 必须通信。</p>\n<p>答案就是 Cross-Attention。</p>\n<h3 id=\"直觉模型翻译官向情报库提问\">直觉模型：翻译官向情报库提问<a class=\"heading-anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#直觉模型翻译官向情报库提问\">#</a></h3>\n<p>Encoder 把源文本编码成 <span data-rehype-pretty-code-figure=\"\"><code data-language=\"text\" data-theme=\"github-light github-dark\" style=\"--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e\"><span data-line=\"\"><span>enc_memory</span></span></code></span>，Decoder 在生成下一个词时，基于当前状态形成 Query，再去 <span data-rehype-pretty-code-figure=\"\"><code data-language=\"text\" data-theme=\"github-light github-dark\" style=\"--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e\"><span data-line=\"\"><span>enc_memory</span></span></code></span> 检索相关信息。</p>\n<figure data-rehype-pretty-code-figure=\"\"><pre style=\"--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e\" tabindex=\"0\" data-language=\"python\" data-theme=\"github-light github-dark\"><code data-language=\"python\" data-theme=\"github-light github-dark\" style=\"display: grid;\"><span data-line=\"\"><span style=\"--shiki-light:#6A737D;--shiki-dark:#6A737D\"># Decoder Cross-Attention</span></span>\n<span data-line=\"\"><span style=\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\">cross_q </span><span style=\"--shiki-light:#D73A49;--shiki-dark:#F97583\">=</span><span style=\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\"> linear(x, state_dict[</span><span style=\"--shiki-light:#D73A49;--shiki-dark:#F97583\">f</span><span style=\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\">'dec_layer</span><span style=\"--shiki-light:#005CC5;--shiki-dark:#79B8FF\">{</span><span style=\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\">li</span><span style=\"--shiki-light:#005CC5;--shiki-dark:#79B8FF\">}</span><span style=\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\">.cross_wq'</span><span style=\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\">])</span></span>\n<span data-line=\"\"><span style=\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\">enc_k_seq </span><span style=\"--shiki-light:#D73A49;--shiki-dark:#F97583\">=</span><span style=\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\"> [linear(mem, state_dict[</span><span style=\"--shiki-light:#D73A49;--shiki-dark:#F97583\">f</span><span style=\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\">'dec_layer</span><span style=\"--shiki-light:#005CC5;--shiki-dark:#79B8FF\">{</span><span style=\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\">li</span><span style=\"--shiki-light:#005CC5;--shiki-dark:#79B8FF\">}</span><span style=\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\">.cross_wk'</span><span style=\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\">]) </span><span style=\"--shiki-light:#D73A49;--shiki-dark:#F97583\">for</span><span style=\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\"> mem </span><span style=\"--shiki-light:#D73A49;--shiki-dark:#F97583\">in</span><span style=\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\"> enc_memory]</span></span>\n<span data-line=\"\"><span style=\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\">enc_v_seq </span><span style=\"--shiki-light:#D73A49;--shiki-dark:#F97583\">=</span><span style=\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\"> [linear(mem, state_dict[</span><span style=\"--shiki-light:#D73A49;--shiki-dark:#F97583\">f</span><span style=\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\">'dec_layer</span><span style=\"--shiki-light:#005CC5;--shiki-dark:#79B8FF\">{</span><span style=\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\">li</span><span style=\"--shiki-light:#005CC5;--shiki-dark:#79B8FF\">}</span><span style=\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\">.cross_wv'</span><span style=\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\">]) </span><span style=\"--shiki-light:#D73A49;--shiki-dark:#F97583\">for</span><span style=\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\"> mem </span><span style=\"--shiki-light:#D73A49;--shiki-dark:#F97583\">in</span><span style=\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\"> enc_memory]</span></span></code></pre></figure>\n<p>关键点：Q、K、V 可以来自不同来源并经过不同投影矩阵。</p>\n<ul>\n<li><span data-rehype-pretty-code-figure=\"\"><code data-language=\"text\" data-theme=\"github-light github-dark\" style=\"--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e\"><span data-line=\"\"><span>Q</span></span></code></span>：来自 Decoder 当前状态（当前“想问什么”）</li>\n<li><span data-rehype-pretty-code-figure=\"\"><code data-language=\"text\" data-theme=\"github-light github-dark\" style=\"--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e\"><span data-line=\"\"><span>K</span></span></code></span>：来自 Encoder Memory（“怎么索引”）</li>\n<li><span data-rehype-pretty-code-figure=\"\"><code data-language=\"text\" data-theme=\"github-light github-dark\" style=\"--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e\"><span data-line=\"\"><span>V</span></span></code></span>：来自 Encoder Memory（“具体拿什么内容”）</li>\n</ul>\n<p>Attention 的本质是：把不同特征映射到可匹配的语义空间，再按权重提取信息。</p>\n<h2 id=\"盲区二为什么-mlp-不会破坏-attention-的成果\">盲区二：为什么 MLP 不会破坏 Attention 的成果？<a class=\"heading-anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#盲区二为什么-mlp-不会破坏-attention-的成果\">#</a></h2>\n<p>每层 Transformer 常见流程是：</p>\n<ol>\n<li>Self-Attention：token 间信息交互</li>\n<li>MLP：token 内部的非线性变换</li>\n</ol>\n<p>常见疑问：Attention 好不容易建立了上下文关系，MLP 会不会把它打散？</p>\n<p>答案是不会，核心有两层原因。</p>\n<h3 id=\"原因-1高维空间中的增量表达\">原因 1：高维空间中的增量表达<a class=\"heading-anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#原因-1高维空间中的增量表达\">#</a></h3>\n<p>Token 表示是高维向量。Attention 可能强化了某些维度上的关系，MLP 更像是在其他维度补充更抽象、更非线性的特征，而不是简单“覆盖旧信息”。</p>\n<h3 id=\"原因-2残差连接保证信息底本被保留\">原因 2：残差连接保证信息底本被保留<a class=\"heading-anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#原因-2残差连接保证信息底本被保留\">#</a></h3>\n<figure data-rehype-pretty-code-figure=\"\"><pre style=\"--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e\" tabindex=\"0\" data-language=\"python\" data-theme=\"github-light github-dark\"><code data-language=\"python\" data-theme=\"github-light github-dark\" style=\"display: grid;\"><span data-line=\"\"><span style=\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\">x_residual </span><span style=\"--shiki-light:#D73A49;--shiki-dark:#F97583\">=</span><span style=\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\"> x</span></span>\n<span data-line=\"\"><span style=\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\">x </span><span style=\"--shiki-light:#D73A49;--shiki-dark:#F97583\">=</span><span style=\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\"> mlp(x)</span></span>\n<span data-line=\"\"><span style=\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\">x </span><span style=\"--shiki-light:#D73A49;--shiki-dark:#F97583\">=</span><span style=\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\"> x </span><span style=\"--shiki-light:#D73A49;--shiki-dark:#F97583\">+</span><span style=\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\"> x_residual</span></span></code></pre></figure>\n<p>这是“在原有信息上做增量修正”，不是“把原信息替换掉”。</p>\n<p>如果初始向量是 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>X</mi><mn>0</mn></msub></mrow><annotation encoding=\"application/x-tex\">X_0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>，那么每层更像是在累积增量：</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi>X</mi><mrow><mi>f</mi><mi>i</mi><mi>n</mi><mi>a</mi><mi>l</mi></mrow></msub><mo>=</mo><msub><mi>X</mi><mn>0</mn></msub><mo>+</mo><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">Δ</mi><msub><mi>A</mi><mn>1</mn></msub><mo>+</mo><mi mathvariant=\"normal\">Δ</mi><msub><mi>M</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo><mo>+</mo><mo>⋯</mo><mo>+</mo><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">Δ</mi><msub><mi>A</mi><mi>n</mi></msub><mo>+</mo><mi mathvariant=\"normal\">Δ</mi><msub><mi>M</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">X_{final} = X_0 + (\\Delta A_1 + \\Delta M_1) + \\dots + (\\Delta A_n + \\Delta M_n)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.9694em;vertical-align:-0.2861em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.10764em;\">f</span><span class=\"mord mathnormal mtight\">ina</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.01968em;\">l</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord\">Δ</span><span class=\"mord\"><span class=\"mord mathnormal\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">Δ</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">M</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.109em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6667em;vertical-align:-0.0833em;\"></span><span class=\"minner\">⋯</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord\">Δ</span><span class=\"mord\"><span class=\"mord mathnormal\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">n</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">Δ</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">M</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:-0.109em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">n</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></span>\n<p>其中 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"normal\">Δ</mi><msub><mi>A</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\Delta A_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\">Δ</span><span class=\"mord\"><span class=\"mord mathnormal\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 是第 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6595em;\"></span><span class=\"mord mathnormal\">i</span></span></span></span> 层 Attention 的增量，<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"normal\">Δ</mi><msub><mi>M</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\Delta M_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\">Δ</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">M</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:-0.109em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 是第 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6595em;\"></span><span class=\"mord mathnormal\">i</span></span></span></span> 层 MLP 的增量。</p>\n<h2 id=\"一句话总结\">一句话总结<a class=\"heading-anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#一句话总结\">#</a></h2>\n<p>Transformer 的深层可训练性来自一个很朴素的工程哲学：不要粗暴覆写上一层表示，而是在残差通道上持续做可控增量。</p>"}},"__N_SSG":true}